{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_context('notebook')\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report, roc_curve, log_loss, brier_score_loss, roc_auc_score, make_scorer\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation Function\n",
    "\n",
    "def CrossVal(model):\n",
    "\n",
    "    start_cross_val = timer()\n",
    "\n",
    "    scores = cross_val_score(model, X_train, y_train)\n",
    "\n",
    "    print('Mean Accuracy:',round(scores.mean()*100,2),'%')\n",
    "    print('Accuracy Standard Deviation',round(scores.std()*100,2),'%')\n",
    "\n",
    "    end_cross_val = timer()\n",
    "    print('Cross Validation Time:', round(end_cross_val - start_cross_val,1), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#murders_full = pd.read_csv('C:\\\\Users\\\\Classy\\\\Desktop\\\\murders_short.csv')\n",
    "murders_no_unknowns = pd.read_csv('C:\\\\Users\\\\Classy\\\\Desktop\\\\murders_no_unknowns.csv')\n",
    "#murders_unknown_ethnic = pd.read_csv('C:\\\\Users\\\\Classy\\\\Desktop\\\\murders_no_unknowns_except_VicEthnic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    220148\n",
       "0     88813\n",
       "Name: Solved, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murders_no_unknowns.Solved.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    88813\n",
       "0    88813\n",
       "Name: Solved, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Balance\n",
    "\n",
    "solved_count, unsolved_count = murders_no_unknowns.Solved.value_counts()\n",
    "solved = murders_no_unknowns[murders_no_unknowns.Solved == 1]\n",
    "unsolved = murders_no_unknowns[murders_no_unknowns.Solved == 0]\n",
    "solved_sample = solved.sample(unsolved_count, random_state = 33)\n",
    "\n",
    "murders_no_unknowns = pd.concat([unsolved, solved_sample], axis = 0)\n",
    "\n",
    "murders_no_unknowns.Solved.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Agentype', 'Solved', 'Month', 'Homicide', 'Situation', 'VicAge',\n",
       "       'VicSex', 'VicRace', 'VicEthnic', 'Weapon', 'VicCount', 'MSA',\n",
       "       'OriClearance', 'OriCases', 'WhiteMurderPercent', 'NonHispanicPercent'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "murders_no_unknowns.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Training Set Random Forest\n",
    "\n",
    "model = RandomForestClassifier(n_estimators = 1000,\n",
    "                                  max_depth = 20,\n",
    "                                  random_state = 33,\n",
    "                                  n_jobs = 3)\n",
    "\n",
    "df_categorical = murders_no_unknowns[[\n",
    "                                    'Agentype',\n",
    "                                    'Month',\n",
    "                                    'Homicide',\n",
    "                                    'Situation',\n",
    "                                    'VicSex',\n",
    "                                    'VicRace',\n",
    "                                    'VicEthnic',\n",
    "                                    'Weapon'\n",
    "                                        ]].reset_index(drop = True)\n",
    "\n",
    "df_numerical = murders_no_unknowns[[\n",
    "                                    'VicAge',\n",
    "                                    'VicCount',\n",
    "                                    'OriCases',\n",
    "                                    'WhiteMurderPercent'\n",
    "                                    ]].reset_index(drop = True)\n",
    "                                                    \n",
    "y = pd.DataFrame(murders_no_unknowns.Solved).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df_categorical))\n",
    "print(type(df_numerical))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Time: 1.8 seconds\n"
     ]
    }
   ],
   "source": [
    "start_preprocessing = timer()\n",
    "\n",
    "#Create dummy columns for categorical features\n",
    "model_df = pd.get_dummies(df_categorical,\n",
    "                          drop_first = True)\n",
    "\n",
    "#Add numerical features to model dataframe\n",
    "model_df = pd.concat([model_df, df_numerical], axis = 1)\n",
    "\n",
    "#Scale features\n",
    "X = pd.DataFrame(StandardScaler().fit_transform(model_df), index = model_df.index, columns = model_df.columns)\n",
    "\n",
    "#Create validation test set to reserve for final validation\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, stratify = y, random_state = 33)\n",
    "\n",
    "end_preprocessing = timer()\n",
    "print('Preprocessing Time:', round(end_preprocessing - start_preprocessing,1), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(X_train.shape)\n",
    "print(X_validation.shape)\n",
    "print(y_train.shape)\n",
    "print(type(X_train))\n",
    "print(type(X_validation))\n",
    "print(type(y_validation))\n",
    "print(X_validation[0:5])\n",
    "print(y_validation[0:5])\n",
    "print(X_train[0:5])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train model\n",
    "start_training = timer()\n",
    "\n",
    "model.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "end_training = timer()\n",
    "print('Training Time:', round(end_training - start_training,1), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use trained model to predict test results\n",
    "start_prediction = timer()\n",
    "\n",
    "y_pred = pd.DataFrame(model.predict(X_validation), index = X_validation.index)\n",
    "y_pred_prob = pd.DataFrame(model.predict_proba(X_validation)[:,1], index = X_validation.index)\n",
    "\n",
    "end_prediction = timer()\n",
    "print('Prediction Time:', round(end_prediction - start_prediction,1), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(y_pred))\n",
    "print(type(y_pred_prob))\n",
    "print(y_pred_prob.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Output test reports\n",
    "start_reports = timer()\n",
    "\n",
    "print('Confusion Matrix:\\n', confusion_matrix(y_validation, y_pred))\n",
    "print('Classification Report:\\n', classification_report(y_validation, y_pred))\n",
    "print('Accuracy:', round((sum(y_pred.values == y_validation.values) / len(y_pred.values) * 100)[0],2), \"%\")\n",
    "print('Log Loss:', log_loss(y_validation, y_pred_prob))\n",
    "print('Brier Score Loss:', brier_score_loss(y_validation, y_pred_prob))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_validation, y_pred_prob)\n",
    "\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n",
    "print('ROC AUC Score:', roc_auc_score(y_validation, y_pred_prob))\n",
    "\n",
    "forest_feature_importance = pd.DataFrame(zip(model_df.columns,model.feature_importances_))\n",
    "forest_feature_importance.sort_values(1, inplace = True)\n",
    "forest_feature_importance.reset_index(inplace = True, drop = True)\n",
    "\n",
    "plt.bar(forest_feature_importance[0][-20:],forest_feature_importance[1][-20:])\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.title('20 Largest Importances')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(forest_feature_importance[0][0:20],forest_feature_importance[1][0:20])\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.title('20 Smallest Importances')\n",
    "plt.show()\n",
    "\n",
    "proba_df = y_validation.copy()\n",
    "proba_df['Predicted Probability'] = y_pred_prob\n",
    "\n",
    "#The below bins by Predicted Probability\n",
    "bins = np.arange(0,1.05,0.05)\n",
    "labels = ['0.05','0.1','0.15','0.2','0.25','0.3','0.35','0.4','0.45','0.5',\n",
    "          '0.55','0.6','0.65','0.7','0.75','0.8','0.85','0.9','0.95','1']\n",
    "proba_df['Bin'] = pd.cut(proba_df['Predicted Probability'], bins = bins, labels = labels)\n",
    "#proba_df.reset_index(drop = True, inplace = True)\n",
    "chart_df = proba_df.groupby('Bin')['Solved', 'Predicted Probability'].mean()\n",
    "chart_df['Count'] = proba_df.groupby('Bin')['Solved'].count()\n",
    "chart_df.reset_index(inplace = True)\n",
    "chart_df.columns = ['Bin', 'Actual', 'Predicted', 'Count']\n",
    "\n",
    "print(chart_df)\n",
    "\n",
    "plt.bar(chart_df.Bin, chart_df.Count)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title('Frequency of Test Cases by Predicted Clearance Probability')\n",
    "plt.xlabel('Case Clearance Probability')\n",
    "plt.ylabel('Number of Cases')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(chart_df.Bin, chart_df.Actual, label = 'Actual', alpha = 1)\n",
    "plt.scatter(chart_df.Bin, chart_df.Predicted, color = 'r', marker = 'D', label = 'Predicted', alpha = 0.75)\n",
    "plt.legend()\n",
    "plt.title('Actual and Predicted Clearance Rate by 5% Bin')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel('Bin')\n",
    "plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])\n",
    "plt.ylabel('Clearance Rate')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(chart_df.Predicted, chart_df.Actual)\n",
    "plt.plot([0,1],[0,1], linestyle = '--', color = 'g')\n",
    "plt.title('Actual vs. Predicted Clearance Rate by 5% bin')\n",
    "plt.xticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1],rotation = 45)\n",
    "plt.xlabel('Predicted')\n",
    "plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1])\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(chart_df.Bin, chart_df.Actual - chart_df.Predicted,)\n",
    "plt.title('Actual - Predicted Clearance Rate by 5% Bin')\n",
    "plt.xticks(rotation = 45)\n",
    "plt.xlabel('Bin (Range = Value - 5% to Value)')\n",
    "plt.yticks([-0.05,-0.04,-0.03,-0.02,-0.01,0,0.01,0.02,0.03,0.04,0.05])\n",
    "plt.ylabel('Delta Clearance Rate')\n",
    "plt.show()\n",
    "\n",
    "print('Sum of Squared Binned Residuals:', binned_sum_of_squared_residuals(y_validation.values, y_pred_prob))\n",
    "\n",
    "end_reports = timer()\n",
    "print('Reporting Time:', round(end_reports - start_reports,1), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function\n",
    "\n",
    "def binned_sum_of_squared_residuals(y_actual, y_predicted):\n",
    "\n",
    "    df = pd.DataFrame(y_actual)\n",
    "    df.columns = ['Actual']\n",
    "    df['Predicted Probability'] = y_predicted\n",
    "\n",
    "    bins = np.arange(0,1.05,0.05)\n",
    "    labels = ['0.05','0.1','0.15','0.2','0.25','0.3','0.35','0.4','0.45','0.5',\n",
    "              '0.55','0.6','0.65','0.7','0.75','0.8','0.85','0.9','0.95','1']\n",
    "    df['Bin'] = pd.cut(df['Predicted Probability'], bins = bins, labels = labels)\n",
    "    bin_df = df.groupby('Bin')['Actual', 'Predicted Probability'].mean()\n",
    "    bin_df.reset_index(inplace = True)\n",
    "    bin_df.columns = ['Bin', 'Actual', 'Predicted']\n",
    "\n",
    "    binned_sum_of_squares = sum(((bin_df.Actual - bin_df.Predicted).fillna(0)*100) ** 2)\n",
    "    \n",
    "    return binned_sum_of_squares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Binned Sum of Squared Binned Residuals:', binned_sum_of_squared_residuals(y_validation, y_pred_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_squared_loss = make_scorer(binned_sum_of_squared_residuals, \n",
    "                                  greater_is_better = False,\n",
    "                                  needs_proba = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [ -478.81520785  -462.17076923  -615.36729123 -8044.48441743\n",
      " -7841.71252911]\n",
      "Mean: -3488.51\n",
      "Standard Deviation 3638.11\n",
      "Cross Validation Time: 4.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Classy\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#Add multiple scoring!!\n",
    "#def cross_val_custom_scoring(model, scoring):\n",
    "\n",
    "start_cross_val = timer()\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, scoring = binned_squared_loss)\n",
    "\n",
    "print('Scores:', scores)\n",
    "print('Mean:', round(scores.mean(), 2))\n",
    "print('Standard Deviation', round(scores.std(), 2))\n",
    "\n",
    "end_cross_val = timer()\n",
    "print('Cross Validation Time:', round(end_cross_val - start_cross_val,1), 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train-test a few model types and use custom scoring function and updated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gridsearch best model (likely Random Forest) using custom scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  25 out of  25 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='auto',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=None, solver='lbfgs',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='deprecated', n_jobs=3,\n",
       "             param_grid=[{'C': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05]}],\n",
       "             pre_dispatch='2*n_jobs', refit=False, return_train_score=False,\n",
       "             scoring={'accuracy': 'accuracy',\n",
       "                      'binned squared loss': make_scorer(binned_sum_of_squared_residuals, greater_is_better=False, needs_proba=True),\n",
       "                      'f1': 'f1'},\n",
       "             verbose=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = LogisticRegression()\n",
    "\n",
    "grid = [{'C': [.000000001, .00000001, .0000001, .000001, .00001]}]\n",
    "\n",
    "scoring = {'f1' : 'f1',\n",
    "           'accuracy' : 'accuracy',\n",
    "           'binned squared loss' : binned_squared_loss}\n",
    "\n",
    "grid_search = GridSearchCV(model, \n",
    "                           param_grid = grid, \n",
    "                           scoring = scoring, \n",
    "                           n_jobs = 3, \n",
    "                           verbose = True, \n",
    "                           return_train_score = False, \n",
    "                           refit = False)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>split2_test_f1</th>\n",
       "      <th>split3_test_f1</th>\n",
       "      <th>...</th>\n",
       "      <th>std_test_accuracy</th>\n",
       "      <th>rank_test_accuracy</th>\n",
       "      <th>split0_test_binned squared loss</th>\n",
       "      <th>split1_test_binned squared loss</th>\n",
       "      <th>split2_test_binned squared loss</th>\n",
       "      <th>split3_test_binned squared loss</th>\n",
       "      <th>split4_test_binned squared loss</th>\n",
       "      <th>mean_test_binned squared loss</th>\n",
       "      <th>std_test_binned squared loss</th>\n",
       "      <th>rank_test_binned squared loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.290134</td>\n",
       "      <td>0.026874</td>\n",
       "      <td>0.058518</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>1e-09</td>\n",
       "      <td>{'C': 1e-09}</td>\n",
       "      <td>0.561473</td>\n",
       "      <td>0.564721</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>0.568541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>5</td>\n",
       "      <td>-158.274405</td>\n",
       "      <td>-178.420091</td>\n",
       "      <td>-179.132143</td>\n",
       "      <td>-172.752129</td>\n",
       "      <td>-176.482367</td>\n",
       "      <td>-173.012227</td>\n",
       "      <td>7.694300</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.310885</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>0.063047</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>1e-08</td>\n",
       "      <td>{'C': 1e-08}</td>\n",
       "      <td>0.561473</td>\n",
       "      <td>0.564744</td>\n",
       "      <td>0.572477</td>\n",
       "      <td>0.568541</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>4</td>\n",
       "      <td>-158.102790</td>\n",
       "      <td>-178.395828</td>\n",
       "      <td>-178.953055</td>\n",
       "      <td>-172.575624</td>\n",
       "      <td>-176.610265</td>\n",
       "      <td>-172.927512</td>\n",
       "      <td>7.741686</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.321437</td>\n",
       "      <td>0.029114</td>\n",
       "      <td>0.063089</td>\n",
       "      <td>0.003945</td>\n",
       "      <td>1e-07</td>\n",
       "      <td>{'C': 1e-07}</td>\n",
       "      <td>0.561767</td>\n",
       "      <td>0.564236</td>\n",
       "      <td>0.572387</td>\n",
       "      <td>0.568756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>3</td>\n",
       "      <td>-157.811863</td>\n",
       "      <td>-175.736469</td>\n",
       "      <td>-177.059276</td>\n",
       "      <td>-172.007648</td>\n",
       "      <td>-176.303633</td>\n",
       "      <td>-171.783778</td>\n",
       "      <td>7.199281</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.336754</td>\n",
       "      <td>0.025163</td>\n",
       "      <td>0.058616</td>\n",
       "      <td>0.004228</td>\n",
       "      <td>1e-06</td>\n",
       "      <td>{'C': 1e-06}</td>\n",
       "      <td>0.562193</td>\n",
       "      <td>0.564486</td>\n",
       "      <td>0.573393</td>\n",
       "      <td>0.568672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>2</td>\n",
       "      <td>-144.888966</td>\n",
       "      <td>-163.875878</td>\n",
       "      <td>-166.642819</td>\n",
       "      <td>-158.496002</td>\n",
       "      <td>-165.120098</td>\n",
       "      <td>-159.804753</td>\n",
       "      <td>7.947070</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.343942</td>\n",
       "      <td>0.059157</td>\n",
       "      <td>0.052397</td>\n",
       "      <td>0.008405</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>{'C': 1e-05}</td>\n",
       "      <td>0.567077</td>\n",
       "      <td>0.568971</td>\n",
       "      <td>0.577262</td>\n",
       "      <td>0.573049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>1</td>\n",
       "      <td>-1767.270985</td>\n",
       "      <td>-2479.764444</td>\n",
       "      <td>-2532.691853</td>\n",
       "      <td>-1875.967586</td>\n",
       "      <td>-2335.686513</td>\n",
       "      <td>-2198.276276</td>\n",
       "      <td>316.101057</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0       0.290134      0.026874         0.058518        0.004246   1e-09   \n",
       "1       0.310885      0.030446         0.063047        0.004093   1e-08   \n",
       "2       0.321437      0.029114         0.063089        0.003945   1e-07   \n",
       "3       0.336754      0.025163         0.058616        0.004228   1e-06   \n",
       "4       0.343942      0.059157         0.052397        0.008405   1e-05   \n",
       "\n",
       "         params  split0_test_f1  split1_test_f1  split2_test_f1  \\\n",
       "0  {'C': 1e-09}        0.561473        0.564721        0.572477   \n",
       "1  {'C': 1e-08}        0.561473        0.564744        0.572477   \n",
       "2  {'C': 1e-07}        0.561767        0.564236        0.572387   \n",
       "3  {'C': 1e-06}        0.562193        0.564486        0.573393   \n",
       "4  {'C': 1e-05}        0.567077        0.568971        0.577262   \n",
       "\n",
       "   split3_test_f1  ...  std_test_accuracy  rank_test_accuracy  \\\n",
       "0        0.568541  ...           0.002140                   5   \n",
       "1        0.568541  ...           0.002150                   4   \n",
       "2        0.568756  ...           0.002025                   3   \n",
       "3        0.568672  ...           0.002266                   2   \n",
       "4        0.573049  ...           0.001900                   1   \n",
       "\n",
       "   split0_test_binned squared loss  split1_test_binned squared loss  \\\n",
       "0                      -158.274405                      -178.420091   \n",
       "1                      -158.102790                      -178.395828   \n",
       "2                      -157.811863                      -175.736469   \n",
       "3                      -144.888966                      -163.875878   \n",
       "4                     -1767.270985                     -2479.764444   \n",
       "\n",
       "   split2_test_binned squared loss  split3_test_binned squared loss  \\\n",
       "0                      -179.132143                      -172.752129   \n",
       "1                      -178.953055                      -172.575624   \n",
       "2                      -177.059276                      -172.007648   \n",
       "3                      -166.642819                      -158.496002   \n",
       "4                     -2532.691853                     -1875.967586   \n",
       "\n",
       "   split4_test_binned squared loss  mean_test_binned squared loss  \\\n",
       "0                      -176.482367                    -173.012227   \n",
       "1                      -176.610265                    -172.927512   \n",
       "2                      -176.303633                    -171.783778   \n",
       "3                      -165.120098                    -159.804753   \n",
       "4                     -2335.686513                   -2198.276276   \n",
       "\n",
       "   std_test_binned squared loss  rank_test_binned squared loss  \n",
       "0                      7.694300                              4  \n",
       "1                      7.741686                              3  \n",
       "2                      7.199281                              2  \n",
       "3                      7.947070                              1  \n",
       "4                    316.101057                              5  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.591252\n",
       "1    0.591275\n",
       "2    0.591425\n",
       "3    0.592453\n",
       "4    0.599059\n",
       "Name: mean_test_accuracy, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_search.cv_results_).mean_test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I THINK that the frequency histogram would ideally be more evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([proba_df,\n",
    "                        df_categorical,\n",
    "                        df_numerical],\n",
    "                       axis = 1)\n",
    "print(results_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_breakdown = results_df.groupby(['Bin','VicRace'])['VicRace'].count()\n",
    "bin_race_breakdown = bin_breakdown.unstack()\n",
    "bin_race_breakdown['Total'] = results_df.groupby('Bin')['VicRace'].count()\n",
    "bin_race_breakdown['Black Percent'] = bin_race_breakdown.Black / bin_race_breakdown.Total\n",
    "bin_race_breakdown['Native Percent'] = bin_race_breakdown['American Indian or Alaskan Native'] / bin_race_breakdown.Total\n",
    "bin_race_breakdown['Asian Percent'] = bin_race_breakdown.Asian / bin_race_breakdown.Total\n",
    "bin_race_breakdown['Native Islander Percent'] = bin_race_breakdown['Native Hawaiian or Pacific Islander'] / bin_race_breakdown.Total\n",
    "bin_race_breakdown['White Percent'] = bin_race_breakdown.White / bin_race_breakdown.Total\n",
    "bin_race_breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_race_breakdown.index, bin_race_breakdown['Black Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_race_breakdown.index, bin_race_breakdown['White Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_breakdown = results_df.groupby('Bin')['OriCases'].mean()\n",
    "plt.bar(bin_breakdown.index, bin_breakdown)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title('Predicted Clearance Probability by Total Number of Cases Handled by Investigating Agency')\n",
    "plt.xlabel('Predicted Clearance Probability')\n",
    "plt.ylabel('Total Number of Cases Handled by Investigating Agency (Bin Average)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWESOME!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_breakdown = results_df.groupby('Bin')['VicAge'].mean()\n",
    "plt.bar(bin_breakdown.index, bin_breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_breakdown = results_df.groupby('Bin')['WhiteMurderPercent'].mean()\n",
    "plt.bar(bin_breakdown.index, bin_breakdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_breakdown = results_df.groupby(['Bin','VicSex'])['VicSex'].count()\n",
    "bin_sex_breakdown = bin_breakdown.unstack()\n",
    "bin_sex_breakdown['Total'] = results_df.groupby('Bin')['VicSex'].count()\n",
    "bin_sex_breakdown['Male Percent'] = bin_sex_breakdown.Male / bin_sex_breakdown.Total\n",
    "bin_sex_breakdown['Female Percent'] = bin_race_breakdown.Female / bin_race_breakdown.Total\n",
    "bin_sex_breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some logreg lines to the above charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe try a smaller number of bins and see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! Try validation on an unbalanced (or at least differently balanced) dataset and see what the results look like"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
